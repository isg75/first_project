{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c97739-9533-47c5-a363-21a427491c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd5cddf-6eb3-47a6-8fc9-4e4bfa42f8d3",
   "metadata": {},
   "source": [
    "Checking out the dataset that was in two parts and then merging them with the concat() method, since they have the same columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f96e6b3-3bed-4c19-8a83-e69c6d0c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df_pt1 = pd.read_csv('../data/raw/df_final_web_data_pt_1.txt')\n",
    "big_df_pt2 = pd.read_csv('../data/raw/df_final_web_data_pt_2.txt')\n",
    "big_df_pt1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c09939-5cf1-4b3e-abcc-2013d21585f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df_pt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814eb87c-ffe7-4bad-8106-529275320ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([big_df_pt1, big_df_pt2], axis=0)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a599b54-0802-492f-a253-0718512f4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic inspecting of the dataframe\n",
    "def inspect_dataframe(merged_df):\n",
    "    \"\"\"\n",
    "    Function to perform basic inspection on a DataFrame: \n",
    "    shape, column names, data types, and missing values.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    print('Check the shape (rows, columns):')\n",
    "    print(merged_df.shape)\n",
    "\n",
    "    print('\\nColumn names:')\n",
    "    print(merged_df.columns)\n",
    "\n",
    "\n",
    "    print('\\nData types:')\n",
    "    print(merged_df.dtypes)\n",
    "\n",
    "\n",
    "    print('\\nMissing values:')\n",
    "    print(merged_df.isnull().sum())\n",
    "\n",
    "# clean column names\n",
    "def clean_column_names(merged_df):\n",
    "    \"\"\"\n",
    "    Function to clean the column names of a DataFrame:\n",
    "    - Convert to lowercase\n",
    "    - Replace spaces with underscores\n",
    "    - Remove or replace special characters with underscores\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_name(name):\n",
    "        name = name.lower()\n",
    "        name = name.replace(\" \", \"_\")\n",
    "        name = re.sub(r'[^a-z0-9_]', '_', name)\n",
    "        return name\n",
    "    \n",
    "    merged_df.columns = [clean_name(col) for col in merged_df.columns]    \n",
    "    return merged_df\n",
    "\n",
    "# check unique and empty values\n",
    "def check_unique_and_empty(merged_df):\n",
    "    \"\"\"\n",
    "    Function to print the unique and empty values for each column in a DataFrame.\n",
    "    \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    \n",
    "    for column in merged_df.columns:\n",
    "        unique_values = merged_df[column].nunique()\n",
    "        empty_values = merged_df[column].isna().sum()\n",
    "        \n",
    "        empty_rows = merged_df[column][merged_df[column].isna()].index.tolist()\n",
    "        \n",
    "        result.append({\n",
    "            'Column': column,\n",
    "            'Unique Values': unique_values,\n",
    "            'Empty Values': empty_values,\n",
    "            'Empty Row Indices': empty_rows\n",
    "        })\n",
    "    \n",
    "    merged_df = pd.DataFrame(result)\n",
    "    \n",
    "    merged_df.set_index('Column', inplace=True)\n",
    "    \n",
    "print(inspect_dataframe(merged_df)) \n",
    "print(clean_column_names(merged_df))\n",
    "print(check_unique_and_empty(merged_df))\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6109e6b-3640-4670-8967-b1dd480a240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0fdc7-19fa-4b2d-bdae-88e0edbf77d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the date_time column values from objects to \n",
    "merged_df['date_time'] = pd.to_datetime(merged_df['date_time'])\n",
    "merged_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e91f48-e4e7-4f49-971f-33be9c785384",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df= merged_df.sort_values(by=[\"client_id\", \"visit_id\", \"date_time\"])\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe34d1-faf4-4a45-95f3-9e9eb08c9287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify 'start' steps that are followed by a 'step_1'\n",
    "# First, shift the process_step column to check the next step\n",
    "merged_df['next_step'] = merged_df.groupby('visit_id')['process_step'].shift(-1)\n",
    "\n",
    "# Step 2: Filter the DataFrame to retain 'start' only if the next step is 'step_1'\n",
    "filtered_df = merged_df[\n",
    "    (merged_df['process_step'] != 'start') | (merged_df['next_step'] == 'step_1')\n",
    "]\n",
    "\n",
    "# Step 3: Drop the temporary 'next_step' column\n",
    "filtered_df = filtered_df.drop(columns=['next_step'])\n",
    "\n",
    "# Step 4: Drop duplicates if needed\n",
    "filtered_df = filtered_df.drop_duplicates()\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92717219-f1c1-43ae-812a-65ee287404e8",
   "metadata": {},
   "source": [
    "### For our project, we need to know how much time a client spends on each step and if they ever reached the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22341ab0-5f5f-4fc8-99f1-9429985ed6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid steps order\n",
    "valid_steps_order = ['start', 'step_1', 'step_2', 'step_3', 'confirm']\n",
    "\n",
    "# Initialize a list to store the results\n",
    "time_spent = []\n",
    "\n",
    "# Function to map steps to their order\n",
    "def step_order(step):\n",
    "    return valid_steps_order.index(step)\n",
    "\n",
    "# Iterate over each unique combination of client, visitor, and visit\n",
    "for (client_id, visitor_id, visit_id), group in filtered_df.groupby(['client_id', 'visitor_id', 'visit_id']):\n",
    "    prev_time = None\n",
    "    prev_step = None\n",
    "    last_valid_step_order = -1  # Start with an invalid step order to force a check for the first step\n",
    "    \n",
    "    for _, row in group.iterrows():\n",
    "        current_time = row['date_time']\n",
    "        current_step = row['process_step']\n",
    "        \n",
    "        # Initialize is_error to False\n",
    "        is_error = False\n",
    "        \n",
    "        # Check if the current step is a valid step and follows the correct order (no skipping)\n",
    "        current_step_order = step_order(current_step)\n",
    "        \n",
    "        if prev_time is not None:\n",
    "            # Calculate the time difference between steps\n",
    "            time_diff = current_time - prev_time\n",
    "            \n",
    "            if current_step_order < last_valid_step_order:\n",
    "                # If the current step goes backwards, just include the row (no error)\n",
    "                time_spent.append({\n",
    "                    'client_id': client_id,\n",
    "                    'visitor_id': visitor_id,\n",
    "                    'visit_id': visit_id,\n",
    "                    'from_step': prev_step,\n",
    "                    'to_step': current_step,\n",
    "                    'time_spent': time_diff,\n",
    "                    'is_error': True  # No error for backward steps\n",
    "                })\n",
    "            elif current_step_order > last_valid_step_order + 1:\n",
    "                # If the step order skips (e.g., step_1 â†’ step_3), **skip this row**\n",
    "                continue  # Skip this row, don't append it\n",
    "            elif current_step_order == last_valid_step_order:\n",
    "                # If the steps to and from are the same, **skip this row**\n",
    "                continue\n",
    "            else:\n",
    "                # Valid transition, append to the list\n",
    "                time_spent.append({\n",
    "                    'client_id': client_id,\n",
    "                    'visitor_id': visitor_id,\n",
    "                    'visit_id': visit_id,\n",
    "                    'from_step': prev_step,\n",
    "                    'to_step': current_step,\n",
    "                    'time_spent': time_diff,\n",
    "                    'is_error': False  # Valid transition, no error\n",
    "                })\n",
    "        \n",
    "        # Update the last valid step order and previous time/step for the next iteration\n",
    "        last_valid_step_order = current_step_order\n",
    "        prev_time = current_time\n",
    "        prev_step = current_step\n",
    "        \n",
    "        if prev_step is None:\n",
    "            # For the first step, no previous step\n",
    "            time_spent.append({\n",
    "                'client_id': client_id,\n",
    "                'visitor_id': visitor_id,\n",
    "                'visit_id': visit_id,\n",
    "                'from_step': None,\n",
    "                'to_step': current_step,\n",
    "                'time_spent': None,\n",
    "                'is_error': False  # No error for the first step\n",
    "            })\n",
    "\n",
    "# Create a DataFrame with the results\n",
    "time_spent_df = pd.DataFrame(time_spent)\n",
    "\n",
    "# Drop rows with None values in 'from_step' and 'to_step'\n",
    "time_spent_df = time_spent_df.dropna(subset=['from_step', 'to_step'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "time_spent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d124386b-f235-4f3c-8ca0-4bd7a90159ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_occured = time_spent_df['is_error'].value_counts()\n",
    "errors_occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e1426-1a20-467a-bbc7-f48fd59dbcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter transitions to include only those that follow the valid steps order\n",
    "valid_transitions = []\n",
    "\n",
    "# Iterate over the time_spent_df and filter valid transitions\n",
    "for _, row in time_spent_df.iterrows():\n",
    "    from_step = row['from_step']\n",
    "    to_step = row['to_step']\n",
    "    \n",
    "    # Check if the transition follows the valid steps order (i.e., from a step to the next one in order)\n",
    "    if from_step is not None and to_step is not None:\n",
    "        if valid_steps_order.index(to_step) == valid_steps_order.index(from_step) + 1:\n",
    "            valid_transitions.append(row)\n",
    "\n",
    "# Create a DataFrame with only valid transitions\n",
    "valid_transitions_df = pd.DataFrame(valid_transitions)\n",
    "\n",
    "# Calculate the average time spent on each transition\n",
    "avg_time_spent = valid_transitions_df.groupby(['from_step', 'to_step'])['time_spent'].mean().round(0).reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "avg_time_spent = avg_time_spent.rename(columns={'time_spent': 'avg_time_spent'})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "avg_time_spent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb18f25-45a7-4562-b538-f9516c2aeb9e",
   "metadata": {},
   "source": [
    "Some clients didn't ever complete the process. Therefore, we need to ensure they are identified and see how many clients reached which step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a72a0f-bb07-4800-96bd-22192cb2e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify clients who didn't complete the whole process (didn't reach \"confirm\")\n",
    "completed_clients = time_spent_df[time_spent_df['to_step'] == 'confirm']['visit_id'].unique()\n",
    "\n",
    "# Identify clients who didn't reach \"confirm\"\n",
    "incomplete_clients = time_spent_df[~time_spent_df['visit_id'].isin(completed_clients)]['visit_id'].unique()\n",
    "\n",
    "# Show which clients are incomplete\n",
    "print(f\"Clients who didn't complete the process: {incomplete_clients}\")\n",
    "\n",
    "# Check the last step they reached\n",
    "last_steps = time_spent_df.groupby('visit_id')['to_step'].last()\n",
    "\n",
    "# Show the last step for each client (whether complete or incomplete)\n",
    "print(f\"Last step for each client: \\n{last_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87be65-8fa7-4e9f-9d2e-bed5fbc14332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of unique visits\n",
    "total_visits = time_spent_df['visit_id'].nunique()\n",
    "\n",
    "# Identify the unique steps, excluding 'start' because it didn't make sense to keep it\n",
    "steps = ['step_1', 'step_2', 'step_3', 'confirm'] \n",
    "\n",
    "# Initialize a dictionary to store the percentage of clients reaching each step\n",
    "step_percentages = {}\n",
    "\n",
    "# Iterate through each step and calculate the percentage\n",
    "for step in steps:\n",
    "    # Find unique visits that reached the step\n",
    "    visits_reached_step = time_spent_df[time_spent_df['to_step'] == step]['visit_id'].nunique()\n",
    "    \n",
    "    # Calculate the percentage of visits who reached this step\n",
    "    step_percentages[step] = (visits_reached_step / total_visits) * 100\n",
    "\n",
    "# Print the results\n",
    "for step, percentage in step_percentages.items():\n",
    "    print(f\"Percentage of visits who reached {step}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9bdb8-1579-4338-aafe-e284e18cdf7e",
   "metadata": {},
   "source": [
    "## Exporting the tables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d35480-b942-4f9c-94db-d04a85dee973",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('../data/clean/web_data_merged_filtered.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345c96b-68d4-483b-b128-827bbef9974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_spent_df.to_csv('../data/clean/time_spent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6176467-e672-41ee-9a63-135c8272545c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
